{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af77bd79-bebd-4ae7-9ff8-516c207b1f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "322673ae",
   "metadata": {},
   "source": [
    "# OpenAI version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "112ed2e4-defa-4122-8502-a8d16375b6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"Write a hello world program in Python\", \"What is the fourth prime number?\"]\n",
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-002\",\n",
    "    prompt=prompt,\n",
    "    temperature=1.0,\n",
    "    max_tokens=64,\n",
    "    top_p=1.0,\n",
    "    frequency_penalty=0.0,\n",
    "    presence_penalty=0.0,\n",
    "    logprobs=1,\n",
    "    echo=True,\n",
    "    n=2,\n",
    "    stop=[\"\\\"\\\"\\\"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "993218e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Write a hello world program in Python\\n\\nprint(\"Hello world!\")',\n",
       " 'Write a hello world program in Python\\n\\nprint(\"Hello, world!\")',\n",
       " 'What is the fourth prime number?\\n\\n7',\n",
       " 'What is the fourth prime number?\\n\\n7']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[response['choices'][i]['text'] for i in range(len(response['choices']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "35c2dc09-938e-409e-89c7-71c2f87e75d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = [response['choices'][i]['logprobs']['token_logprobs'][1:]\n",
    "            for i in range(len(response['choices']))]\n",
    "tokens = [response['choices'][i]['logprobs']['tokens'][1:]\n",
    "        for i in range(len(response['choices']))]\n",
    "offsets = [response['choices'][i]['logprobs']['text_offset'][1:]\n",
    "        for i in range(len(response['choices']))]\n",
    "\n",
    "# Subtract 1 from the offsets to account for the newline\n",
    "for i in range(len(offsets)):\n",
    "    offsets[i] = [offset - 1 for offset in offsets[i]]\n",
    "\n",
    "# if log_prob_range is not None:\n",
    "#     # First, we need to find the indices of the tokens in the log probs\n",
    "#     # that correspond to the tokens in the log_prob_range\n",
    "#     for i in range(len(log_probs)):\n",
    "#         lower_index, upper_index = self.get_token_indices(\n",
    "#             offsets[i], log_prob_range[i])\n",
    "#         log_probs[i] = log_probs[i][lower_index:upper_index]\n",
    "#         tokens[i] = tokens[i][lower_index:upper_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0a45948b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' a',\n",
       " ' hello',\n",
       " ' world',\n",
       " ' program',\n",
       " ' in',\n",
       " ' Python',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'print',\n",
       " '(\"',\n",
       " 'Hello',\n",
       " ' world',\n",
       " '!\"',\n",
       " ')']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "39427d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.3710930645465851,\n",
       " -3.699204444885254,\n",
       " -1.6033620834350586,\n",
       " -0.07942259311676025,\n",
       " -0.0023649362847208977,\n",
       " -0.5106741189956665,\n",
       " -5.155109405517578,\n",
       " -1.1185113191604614,\n",
       " -3.548737049102783,\n",
       " -1.20339035987854,\n",
       " -0.037674229592084885,\n",
       " -0.8867388367652893,\n",
       " -1.3799138069152832,\n",
       " -0.006083305925130844,\n",
       " -0.0009765623253770173,\n",
       " -1.0728830375228426e-06,\n",
       " -0.0003610197745729238,\n",
       " -0.0073336209170520306,\n",
       " -2.0263919830322266,\n",
       " -0.06149892508983612,\n",
       " -0.2051912397146225,\n",
       " -1.7620773315429688,\n",
       " -0.0040281834080815315,\n",
       " -0.8398252725601196,\n",
       " -0.03858637437224388,\n",
       " -0.026370810344815254,\n",
       " -3.7345595359802246,\n",
       " -4.294748783111572,\n",
       " -1.0190117359161377]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "83ae4c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 6, 12, 18, 26, 29, 36, 37, 38, 43, 45, 50, 56, 58]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offsets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6966b9b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 14)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens[0]), len(log_probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354caefb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62f3bb7b-cfa0-492c-acd0-98e688939069",
   "metadata": {},
   "source": [
    "# HuggingFace version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b52254e-4859-483c-a71e-251023404b06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99ffbdd5-dc46-4833-b653-8bf879098137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import numpy as np\n",
    "\n",
    "dev = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f323883-8e5b-4f78-8dcf-52166c314c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289b27eea4524c70be7c5ca3838edf08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xl\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64205eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5TokenizerFast(name_or_path='google/flan-t5-xl', vocab_size=32100, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "087b629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_predict(input_texts, model, tokenizer, n=1, logprobs = False):\n",
    "    \"\"\"\n",
    "    Takes in a list of text inputs and generates output text for each input using the model and tokenizer.\n",
    "\n",
    "    Args:\n",
    "        input_texts (List[str]): List of text inputs to be used as input to the model.\n",
    "        model (transformers.PreTrainedModel): Model to be used for prediction.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): Tokenizer to be used for prediction.\n",
    "        n (int, optional): Number of predictions to generate for each input. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of generated text.\n",
    "    \"\"\"\n",
    "    # Encode the input texts\n",
    "    inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "\n",
    "    # Generate output with the model\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=30,\n",
    "        num_return_sequences=n,\n",
    "        do_sample=True,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True)\n",
    "\n",
    "    transition_scores = model.compute_transition_scores(\n",
    "        outputs.sequences, outputs.scores, normalize_logits=True\n",
    "    )\n",
    "\n",
    "    # ref: https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.compute_transition_scores\n",
    "    # input_length is the length of the input prompt for decoder-only models, like the GPT family, and 1 for\n",
    "    # encoder-decoder models, like BART or T5.\n",
    "    input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "    generated_tokens = outputs.sequences[:, input_length:]\n",
    "\n",
    "    if logprobs:\n",
    "\n",
    "        filtered_tokens = []\n",
    "        filtered_scores = []\n",
    "\n",
    "        # remove padding and eos tokens, which have a score of -inf\n",
    "        for seq_tokens, seq_scores in zip(generated_tokens, transition_scores):\n",
    "            unpadded_tokens = [token for token in seq_tokens if token not in (tokenizer.pad_token_id, tokenizer.eos_token_id)]\n",
    "            unpadded_scores = [score for token, score in zip(seq_tokens, seq_scores) if token not in (tokenizer.pad_token_id, tokenizer.eos_token_id)]\n",
    "            \n",
    "            filtered_tokens.append(unpadded_tokens)\n",
    "            filtered_scores.append(unpadded_scores)\n",
    "\n",
    "        token_seq = [[tokenizer.decode(token_id, clean_up_tokenization_spaces=True, skip_special_tokens=False) for token_id in tokens] for tokens in filtered_tokens]\n",
    "        \n",
    "        return filtered_scores, token_seq    \n",
    "        # token_seq = [[tokenizer.decode(token_id, clean_up_tokenization_spaces = True, skip_special_tokens=False) for token_id in tokens] for tokens in generated_tokens]\n",
    "        # log_probs = [transition_scores[i].tolist() for i in range(transition_scores.shape[0])]\n",
    "        # return log_probs, token_seq\n",
    "\n",
    "    else:\n",
    "        # Convert tokens to text\n",
    "        generated_texts = tokenizer.batch_decode(generated_tokens, clean_up_tokenization_spaces = True, skip_special_tokens=True)\n",
    "\n",
    "        return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "039433bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| import   | -2.755 | 6.36%\n",
      "|          | -0.253 | 77.62%\n",
      "| i        | -3.746 | 2.36%\n",
      "| o        | -0.019 | 98.11%\n",
      "| ,        | -0.807 | 44.63%\n",
      "|          | -0.227 | 79.67%\n",
      "| o        | -0.524 | 59.24%\n",
      "| s        | -0.002 | 99.80%\n",
      "| import   | -2.029 | 13.14%\n",
      "|          | -0.222 | 80.09%\n",
      "| s        | -0.227 | 79.66%\n",
      "| y        | -0.013 | 98.75%\n",
      "| s        | -0.000 | 100.00%\n",
      "| import   | -2.124 | 11.96%\n",
      "|          | -1.383 | 25.07%\n",
      "| re       | -1.154 | 31.54%\n",
      "| ,        | -2.567 | 7.67%\n",
      "|          | -0.388 | 67.83%\n",
      "| s        | -1.048 | 35.08%\n",
      "| y        | -0.744 | 47.53%\n",
      "| s        | -0.002 | 99.82%\n",
      "| .        | -0.246 | 78.20%\n",
      "| set      | -0.434 | 64.80%\n",
      "| re       | -0.000 | 99.97%\n",
      "| cur      | -0.000 | 100.00%\n",
      "| sion     | -0.000 | 100.00%\n",
      "| limit    | -0.000 | 99.99%\n",
      "| (        | -0.006 | 99.39%\n",
      "| 10       | -0.339 | 71.26%\n"
     ]
    }
   ],
   "source": [
    "log_probs, tokens = do_predict([\"Write a hello world program in Python\", \"What is the fourth prime number?\"], model, tokenizer, n=3, logprobs = True)\n",
    "for tok, score in zip(tokens[0], log_probs[0]):\n",
    "    # | token | token string | logits | probability\n",
    "    print(f\"| {tok:8s} | {score.cpu():.3f} | {np.exp(score.cpu()):.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0182dcbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 29)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generated_tokens[0]), len(transition_scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9344c081",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = do_predict([\"Write a hello world program in Python\", \"What is the fourth prime number?\"], model, tokenizer, n=3, logprobs = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0552be8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ok=0 while y==\"\" and y!=0 while y!= 0: y+=',\n",
       " 'i = 1 while i  len(stdin) : i += 1 if i!',\n",
       " 'vf = lambda x, y: [x,y]; vf = vf(); ',\n",
       " 'ten',\n",
       " 'ten',\n",
       " '14']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e676f56e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7989977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20e373a6-835d-4c54-9701-3589f21caebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"For Halloween Debby and her sister combined the candy they received. Debby had 32 pieces of candy while her sister had 42. If they ate 35 pieces the first night, how many pieces do they have left?\"\n",
    "preprompt_1 = \"\"\n",
    "postprompt_1 = \"\"\n",
    "\n",
    "preprompt_2 = \"\"\n",
    "postprompt_2 = \"Let's think step by step.\"\n",
    "\n",
    "input_ids_1 = tokenizer(preprompt_1 + input_text + postprompt_1, return_tensors=\"pt\").input_ids.to(dev)\n",
    "input_ids_2 = tokenizer(preprompt_2 + input_text + postprompt_2, return_tensors=\"pt\").input_ids.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2dd739-f899-49be-8b80-9df2d5619eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920a57b7-5bbe-4431-9388-753830fe985a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pevo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
