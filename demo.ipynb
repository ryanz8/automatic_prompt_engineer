{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Prompts with **Automatic Prompt Engineer** (APE)\n",
    "\n",
    "This notebook demonstrates how to use Automatic Prompt Engineer (APE) (arxiv link) to optimize prompts for text generation. In its simplest form, APE takes as input a dataset (a list of inputs and a list of outputs), a prompt template, and optimizes this prompt template so that it generates the outputs given the inputs.\n",
    "\n",
    "APE accomplishes this in two steps. First, it uses a language model to generate a set of candidate prompts. Then, it uses a prompt evaluation function to evaluate the quality of each candidate prompt. Finally, it returns the prompt with the highest evaluation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's define a simple dataset consisting of words and their antonyms.\n",
    "words = [\"sane\", \"direct\", \"informally\", \"unpopular\", \"subtractive\", \"nonresidential\",\n",
    "    \"inexact\", \"uptown\", \"incomparable\", \"powerful\", \"gaseous\", \"evenly\", \"formality\",\n",
    "    \"deliberately\", \"off\"]\n",
    "antonyms = [\"insane\", \"indirect\", \"formally\", \"popular\", \"additive\", \"residential\",\n",
    "    \"exact\", \"downtown\", \"comparable\", \"powerless\", \"solid\", \"unevenly\", \"informality\",\n",
    "    \"accidentally\", \"on\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we need to define the format of the prompt that we are using.\n",
    "\n",
    "eval_template = \\\n",
    "\"\"\"Instruction: [PROMPT]\n",
    "Input: [INPUT]\n",
    "Output: [OUTPUT]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's use APE to find prompts that generate antonyms for each word.\n",
    "from automatic_prompt_engineer import ape\n",
    "\n",
    "results = ape.simple_ape(\n",
    "    dataset=(words, antonyms),\n",
    "    eval_template=eval_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the results.\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare with a prompt written by a human:\n",
    "\n",
    "\"*Write an antonym to the following word.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automatic_prompt_engineer import ape\n",
    "\n",
    "manual_prompt = \"Write an antonym to the following word.\"\n",
    "\n",
    "human_result = ape.simple_eval(\n",
    "    dataset=(words, antonyms),\n",
    "    eval_template=eval_template,\n",
    "    prompts=[manual_prompt],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(human_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2afcb7a2e6fcb9490d448e607abf9226c3f7acca28baeea9bc24b456562037f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
